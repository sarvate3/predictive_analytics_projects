{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sarva\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               33554944  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 33,686,529\n",
      "Trainable params: 33,686,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 65536)             67174400  \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 256, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 67,864,320\n",
      "Trainable params: 67,860,736\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarva\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:214: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sarva\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarva\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.743513, acc.: 20.00%] [G loss: 0.702563]\n",
      "1 [D loss: 1.298521, acc.: 80.00%] [G loss: 0.592189]\n",
      "2 [D loss: 2.382109, acc.: 55.00%] [G loss: 0.992614]\n",
      "3 [D loss: 0.413194, acc.: 80.00%] [G loss: 2.432961]\n",
      "4 [D loss: 0.952221, acc.: 80.00%] [G loss: 2.844276]\n",
      "5 [D loss: 0.852360, acc.: 80.00%] [G loss: 1.644858]\n",
      "6 [D loss: 2.694592, acc.: 60.00%] [G loss: 2.581427]\n",
      "7 [D loss: 2.069552, acc.: 70.00%] [G loss: 2.472780]\n",
      "8 [D loss: 1.809710, acc.: 70.00%] [G loss: 3.651322]\n",
      "9 [D loss: 0.311621, acc.: 80.00%] [G loss: 5.227997]\n",
      "10 [D loss: 0.722138, acc.: 80.00%] [G loss: 3.212879]\n",
      "11 [D loss: 0.543819, acc.: 85.00%] [G loss: 5.044106]\n",
      "12 [D loss: 0.364367, acc.: 85.00%] [G loss: 5.759973]\n",
      "13 [D loss: 0.555628, acc.: 80.00%] [G loss: 6.754730]\n",
      "14 [D loss: 0.137963, acc.: 90.00%] [G loss: 4.826013]\n",
      "15 [D loss: 0.564941, acc.: 85.00%] [G loss: 4.579136]\n",
      "16 [D loss: 0.868876, acc.: 75.00%] [G loss: 5.488503]\n",
      "17 [D loss: 1.104679, acc.: 80.00%] [G loss: 6.355289]\n",
      "18 [D loss: 0.352595, acc.: 85.00%] [G loss: 5.357312]\n",
      "19 [D loss: 0.773601, acc.: 65.00%] [G loss: 4.771124]\n",
      "20 [D loss: 0.194755, acc.: 95.00%] [G loss: 7.503073]\n",
      "21 [D loss: 0.063024, acc.: 100.00%] [G loss: 5.346088]\n",
      "22 [D loss: 0.194490, acc.: 90.00%] [G loss: 5.205990]\n",
      "23 [D loss: 0.707201, acc.: 80.00%] [G loss: 4.063581]\n",
      "24 [D loss: 0.512203, acc.: 85.00%] [G loss: 7.615877]\n",
      "25 [D loss: 0.598543, acc.: 85.00%] [G loss: 7.014941]\n",
      "26 [D loss: 0.056779, acc.: 100.00%] [G loss: 7.370610]\n",
      "27 [D loss: 0.154359, acc.: 90.00%] [G loss: 3.803814]\n",
      "28 [D loss: 0.116083, acc.: 95.00%] [G loss: 6.644973]\n",
      "29 [D loss: 0.086727, acc.: 95.00%] [G loss: 7.899617]\n",
      "30 [D loss: 0.965204, acc.: 65.00%] [G loss: 5.910372]\n",
      "31 [D loss: 0.261870, acc.: 85.00%] [G loss: 8.078195]\n",
      "32 [D loss: 0.133416, acc.: 95.00%] [G loss: 5.308246]\n",
      "33 [D loss: 0.152018, acc.: 95.00%] [G loss: 9.065197]\n",
      "34 [D loss: 0.131926, acc.: 95.00%] [G loss: 8.868608]\n",
      "35 [D loss: 0.687327, acc.: 90.00%] [G loss: 5.748982]\n",
      "36 [D loss: 1.267318, acc.: 80.00%] [G loss: 9.389372]\n",
      "37 [D loss: 0.099069, acc.: 95.00%] [G loss: 8.115125]\n",
      "38 [D loss: 0.144137, acc.: 95.00%] [G loss: 9.829920]\n",
      "39 [D loss: 0.639633, acc.: 90.00%] [G loss: 10.245333]\n",
      "40 [D loss: 0.406289, acc.: 90.00%] [G loss: 7.405636]\n",
      "41 [D loss: 0.295501, acc.: 85.00%] [G loss: 6.358229]\n",
      "42 [D loss: 0.499845, acc.: 80.00%] [G loss: 9.040442]\n",
      "43 [D loss: 0.045474, acc.: 100.00%] [G loss: 11.448853]\n",
      "44 [D loss: 1.539936, acc.: 60.00%] [G loss: 6.957349]\n",
      "45 [D loss: 1.133758, acc.: 80.00%] [G loss: 10.343557]\n",
      "46 [D loss: 0.992316, acc.: 80.00%] [G loss: 10.531919]\n",
      "47 [D loss: 0.757314, acc.: 80.00%] [G loss: 11.136113]\n",
      "48 [D loss: 0.764377, acc.: 90.00%] [G loss: 7.887468]\n",
      "49 [D loss: 0.430566, acc.: 90.00%] [G loss: 7.112411]\n",
      "50 [D loss: 0.156896, acc.: 95.00%] [G loss: 8.182507]\n",
      "51 [D loss: 0.576887, acc.: 90.00%] [G loss: 8.442793]\n",
      "52 [D loss: 0.115944, acc.: 95.00%] [G loss: 9.016847]\n",
      "53 [D loss: 0.259829, acc.: 95.00%] [G loss: 8.671659]\n",
      "54 [D loss: 0.909236, acc.: 75.00%] [G loss: 3.797025]\n",
      "55 [D loss: 0.754237, acc.: 75.00%] [G loss: 6.504369]\n",
      "56 [D loss: 0.505196, acc.: 85.00%] [G loss: 5.895712]\n",
      "57 [D loss: 0.614225, acc.: 65.00%] [G loss: 6.685346]\n",
      "58 [D loss: 0.203068, acc.: 80.00%] [G loss: 8.034510]\n",
      "59 [D loss: 0.210172, acc.: 85.00%] [G loss: 10.612283]\n",
      "60 [D loss: 0.483173, acc.: 85.00%] [G loss: 10.630300]\n",
      "61 [D loss: 0.990359, acc.: 85.00%] [G loss: 3.415717]\n",
      "62 [D loss: 0.485409, acc.: 80.00%] [G loss: 8.625154]\n",
      "63 [D loss: 1.167458, acc.: 80.00%] [G loss: 9.179768]\n",
      "64 [D loss: 1.368261, acc.: 65.00%] [G loss: 8.870091]\n",
      "65 [D loss: 1.674294, acc.: 70.00%] [G loss: 8.269000]\n",
      "66 [D loss: 1.350740, acc.: 75.00%] [G loss: 8.016142]\n",
      "67 [D loss: 0.261081, acc.: 95.00%] [G loss: 12.166475]\n",
      "68 [D loss: 0.736141, acc.: 85.00%] [G loss: 7.025545]\n",
      "69 [D loss: 0.997551, acc.: 85.00%] [G loss: 7.447267]\n",
      "70 [D loss: 0.504422, acc.: 80.00%] [G loss: 7.538410]\n",
      "71 [D loss: 0.641989, acc.: 85.00%] [G loss: 10.746232]\n",
      "72 [D loss: 0.014645, acc.: 100.00%] [G loss: 11.007457]\n",
      "73 [D loss: 2.041590, acc.: 70.00%] [G loss: 10.844440]\n",
      "74 [D loss: 0.924537, acc.: 80.00%] [G loss: 7.467135]\n",
      "75 [D loss: 0.780098, acc.: 90.00%] [G loss: 8.493231]\n",
      "76 [D loss: 1.304296, acc.: 75.00%] [G loss: 7.448401]\n",
      "77 [D loss: 2.090009, acc.: 70.00%] [G loss: 9.599635]\n",
      "78 [D loss: 0.917266, acc.: 90.00%] [G loss: 12.102241]\n",
      "79 [D loss: 2.901042, acc.: 70.00%] [G loss: 9.396304]\n",
      "80 [D loss: 0.295156, acc.: 90.00%] [G loss: 9.814919]\n",
      "81 [D loss: 1.336797, acc.: 80.00%] [G loss: 11.268372]\n",
      "82 [D loss: 0.838820, acc.: 80.00%] [G loss: 11.659280]\n",
      "83 [D loss: 1.730431, acc.: 85.00%] [G loss: 12.290898]\n",
      "84 [D loss: 0.023053, acc.: 100.00%] [G loss: 8.799376]\n",
      "85 [D loss: 0.244474, acc.: 90.00%] [G loss: 9.476323]\n",
      "86 [D loss: 0.154395, acc.: 95.00%] [G loss: 8.063849]\n",
      "87 [D loss: 0.989462, acc.: 90.00%] [G loss: 7.987076]\n",
      "88 [D loss: 1.600671, acc.: 75.00%] [G loss: 8.204870]\n",
      "89 [D loss: 0.369614, acc.: 85.00%] [G loss: 13.324896]\n",
      "90 [D loss: 0.552336, acc.: 90.00%] [G loss: 10.736727]\n",
      "91 [D loss: 0.226791, acc.: 90.00%] [G loss: 10.455681]\n",
      "92 [D loss: 1.657291, acc.: 80.00%] [G loss: 11.496631]\n",
      "93 [D loss: 0.457490, acc.: 90.00%] [G loss: 9.236996]\n",
      "94 [D loss: 0.855059, acc.: 85.00%] [G loss: 11.144699]\n",
      "95 [D loss: 0.590605, acc.: 90.00%] [G loss: 10.802547]\n",
      "96 [D loss: 0.771353, acc.: 85.00%] [G loss: 8.608593]\n",
      "97 [D loss: 0.239520, acc.: 95.00%] [G loss: 13.709317]\n",
      "98 [D loss: 2.108208, acc.: 70.00%] [G loss: 10.762870]\n",
      "99 [D loss: 0.828219, acc.: 85.00%] [G loss: 14.665087]\n",
      "100 [D loss: 0.675723, acc.: 85.00%] [G loss: 9.377779]\n",
      "101 [D loss: 1.214885, acc.: 80.00%] [G loss: 7.457577]\n",
      "102 [D loss: 0.067065, acc.: 95.00%] [G loss: 11.230906]\n",
      "103 [D loss: 2.661177, acc.: 45.00%] [G loss: 13.910196]\n",
      "104 [D loss: 3.231515, acc.: 75.00%] [G loss: 12.370973]\n",
      "105 [D loss: 0.882149, acc.: 90.00%] [G loss: 11.691020]\n",
      "106 [D loss: 1.105264, acc.: 90.00%] [G loss: 12.474078]\n",
      "107 [D loss: 1.890158, acc.: 85.00%] [G loss: 12.869812]\n",
      "108 [D loss: 2.321059, acc.: 75.00%] [G loss: 12.852971]\n",
      "109 [D loss: 0.609350, acc.: 85.00%] [G loss: 9.712553]\n",
      "110 [D loss: 1.025204, acc.: 80.00%] [G loss: 9.707136]\n",
      "111 [D loss: 1.413162, acc.: 80.00%] [G loss: 10.308998]\n",
      "112 [D loss: 0.361972, acc.: 90.00%] [G loss: 10.261923]\n",
      "113 [D loss: 0.646140, acc.: 85.00%] [G loss: 9.138245]\n",
      "114 [D loss: 0.205526, acc.: 95.00%] [G loss: 10.885996]\n",
      "115 [D loss: 0.424306, acc.: 95.00%] [G loss: 10.618204]\n",
      "116 [D loss: 1.281826, acc.: 80.00%] [G loss: 14.001882]\n",
      "117 [D loss: 1.594493, acc.: 70.00%] [G loss: 7.530431]\n",
      "118 [D loss: 2.663996, acc.: 75.00%] [G loss: 9.151342]\n",
      "119 [D loss: 3.055451, acc.: 60.00%] [G loss: 6.603377]\n",
      "120 [D loss: 0.739241, acc.: 85.00%] [G loss: 7.084020]\n",
      "121 [D loss: 3.543863, acc.: 75.00%] [G loss: 7.999060]\n",
      "122 [D loss: 2.974277, acc.: 75.00%] [G loss: 12.573718]\n",
      "123 [D loss: 1.782866, acc.: 85.00%] [G loss: 8.705890]\n",
      "124 [D loss: 0.853856, acc.: 85.00%] [G loss: 13.021039]\n",
      "125 [D loss: 0.576042, acc.: 85.00%] [G loss: 10.589237]\n",
      "126 [D loss: 0.063024, acc.: 95.00%] [G loss: 11.045476]\n",
      "127 [D loss: 2.156937, acc.: 70.00%] [G loss: 11.454242]\n",
      "128 [D loss: 0.040965, acc.: 100.00%] [G loss: 14.119342]\n",
      "129 [D loss: 0.803159, acc.: 80.00%] [G loss: 11.979526]\n",
      "130 [D loss: 0.806729, acc.: 95.00%] [G loss: 12.894476]\n",
      "131 [D loss: 0.298890, acc.: 95.00%] [G loss: 11.470136]\n",
      "132 [D loss: 1.913732, acc.: 85.00%] [G loss: 11.392720]\n",
      "133 [D loss: 0.923113, acc.: 90.00%] [G loss: 12.520845]\n",
      "134 [D loss: 2.568595, acc.: 65.00%] [G loss: 11.819609]\n",
      "135 [D loss: 1.645478, acc.: 85.00%] [G loss: 7.867759]\n",
      "136 [D loss: 0.004648, acc.: 100.00%] [G loss: 12.516234]\n",
      "137 [D loss: 0.889366, acc.: 80.00%] [G loss: 12.990959]\n",
      "138 [D loss: 0.564876, acc.: 90.00%] [G loss: 13.442276]\n",
      "139 [D loss: 3.065063, acc.: 70.00%] [G loss: 12.738435]\n",
      "140 [D loss: 1.496870, acc.: 85.00%] [G loss: 12.476774]\n",
      "141 [D loss: 0.431552, acc.: 85.00%] [G loss: 12.894518]\n",
      "142 [D loss: 1.151036, acc.: 90.00%] [G loss: 11.314087]\n",
      "143 [D loss: 0.005157, acc.: 100.00%] [G loss: 10.192986]\n",
      "144 [D loss: 1.281079, acc.: 85.00%] [G loss: 10.593918]\n",
      "145 [D loss: 0.104191, acc.: 100.00%] [G loss: 13.848742]\n",
      "146 [D loss: 0.240732, acc.: 95.00%] [G loss: 14.016421]\n",
      "147 [D loss: 0.003507, acc.: 100.00%] [G loss: 12.750544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 [D loss: 0.670746, acc.: 85.00%] [G loss: 12.877521]\n",
      "149 [D loss: 0.966031, acc.: 80.00%] [G loss: 10.003552]\n",
      "150 [D loss: 0.128389, acc.: 90.00%] [G loss: 10.742258]\n",
      "151 [D loss: 0.260562, acc.: 90.00%] [G loss: 9.963778]\n",
      "152 [D loss: 1.928677, acc.: 70.00%] [G loss: 8.244992]\n",
      "153 [D loss: 1.253744, acc.: 80.00%] [G loss: 8.741196]\n",
      "154 [D loss: 1.728269, acc.: 80.00%] [G loss: 8.939075]\n",
      "155 [D loss: 2.239958, acc.: 80.00%] [G loss: 7.777328]\n",
      "156 [D loss: 0.709557, acc.: 80.00%] [G loss: 8.496914]\n",
      "157 [D loss: 0.003705, acc.: 100.00%] [G loss: 10.636057]\n",
      "158 [D loss: 1.956645, acc.: 75.00%] [G loss: 11.583559]\n",
      "159 [D loss: 0.841290, acc.: 85.00%] [G loss: 10.039459]\n",
      "160 [D loss: 1.109788, acc.: 90.00%] [G loss: 8.426184]\n",
      "161 [D loss: 2.587376, acc.: 60.00%] [G loss: 14.507462]\n",
      "162 [D loss: 0.890556, acc.: 80.00%] [G loss: 14.266687]\n",
      "163 [D loss: 1.428982, acc.: 70.00%] [G loss: 14.506699]\n",
      "164 [D loss: 1.578865, acc.: 75.00%] [G loss: 13.030795]\n",
      "165 [D loss: 0.651041, acc.: 95.00%] [G loss: 11.784996]\n",
      "166 [D loss: 1.261131, acc.: 90.00%] [G loss: 12.990768]\n",
      "167 [D loss: 0.865638, acc.: 95.00%] [G loss: 12.340034]\n",
      "168 [D loss: 3.245166, acc.: 50.00%] [G loss: 15.382619]\n",
      "169 [D loss: 0.018328, acc.: 100.00%] [G loss: 14.980034]\n",
      "170 [D loss: 0.580581, acc.: 95.00%] [G loss: 10.721787]\n",
      "171 [D loss: 0.174320, acc.: 90.00%] [G loss: 13.682915]\n",
      "172 [D loss: 0.556939, acc.: 80.00%] [G loss: 13.124896]\n",
      "173 [D loss: 0.390952, acc.: 80.00%] [G loss: 12.202507]\n",
      "174 [D loss: 0.366366, acc.: 85.00%] [G loss: 15.381147]\n",
      "175 [D loss: 1.269031, acc.: 85.00%] [G loss: 15.707834]\n",
      "176 [D loss: 0.691519, acc.: 85.00%] [G loss: 8.321976]\n",
      "177 [D loss: 0.658051, acc.: 85.00%] [G loss: 10.491386]\n",
      "178 [D loss: 0.548724, acc.: 85.00%] [G loss: 10.218561]\n",
      "179 [D loss: 0.103101, acc.: 95.00%] [G loss: 12.295479]\n",
      "180 [D loss: 0.116805, acc.: 95.00%] [G loss: 12.530181]\n",
      "181 [D loss: 0.114456, acc.: 95.00%] [G loss: 13.151682]\n",
      "182 [D loss: 1.621162, acc.: 75.00%] [G loss: 14.237371]\n",
      "183 [D loss: 0.917990, acc.: 90.00%] [G loss: 11.297525]\n",
      "184 [D loss: 2.196528, acc.: 80.00%] [G loss: 14.854068]\n",
      "185 [D loss: 1.128245, acc.: 80.00%] [G loss: 12.909851]\n",
      "186 [D loss: 0.292582, acc.: 85.00%] [G loss: 12.140184]\n",
      "187 [D loss: 0.375558, acc.: 85.00%] [G loss: 8.550385]\n",
      "188 [D loss: 0.017157, acc.: 100.00%] [G loss: 15.231662]\n",
      "189 [D loss: 1.100672, acc.: 85.00%] [G loss: 13.456072]\n",
      "190 [D loss: 1.452820, acc.: 80.00%] [G loss: 11.635080]\n",
      "191 [D loss: 0.078816, acc.: 95.00%] [G loss: 13.750211]\n",
      "192 [D loss: 0.385265, acc.: 95.00%] [G loss: 14.200772]\n",
      "193 [D loss: 0.195740, acc.: 95.00%] [G loss: 13.222033]\n",
      "194 [D loss: 0.327504, acc.: 90.00%] [G loss: 11.148499]\n",
      "195 [D loss: 0.084740, acc.: 95.00%] [G loss: 12.433424]\n",
      "196 [D loss: 0.010941, acc.: 100.00%] [G loss: 12.478694]\n",
      "197 [D loss: 0.823960, acc.: 75.00%] [G loss: 13.717479]\n",
      "198 [D loss: 0.679392, acc.: 85.00%] [G loss: 13.225870]\n",
      "199 [D loss: 0.005051, acc.: 100.00%] [G loss: 13.950624]\n",
      "200 [D loss: 2.183968, acc.: 70.00%] [G loss: 13.511892]\n",
      "201 [D loss: 0.205774, acc.: 90.00%] [G loss: 11.353299]\n",
      "202 [D loss: 4.208348, acc.: 50.00%] [G loss: 11.283710]\n",
      "203 [D loss: 1.386033, acc.: 85.00%] [G loss: 11.041015]\n",
      "204 [D loss: 2.147008, acc.: 80.00%] [G loss: 13.328569]\n",
      "205 [D loss: 0.998122, acc.: 90.00%] [G loss: 12.282495]\n",
      "206 [D loss: 1.663566, acc.: 75.00%] [G loss: 11.569178]\n",
      "207 [D loss: 0.260595, acc.: 95.00%] [G loss: 8.829766]\n",
      "208 [D loss: 0.016214, acc.: 100.00%] [G loss: 10.837804]\n",
      "209 [D loss: 0.705457, acc.: 85.00%] [G loss: 13.185941]\n",
      "210 [D loss: 0.672797, acc.: 85.00%] [G loss: 11.547888]\n",
      "211 [D loss: 0.626166, acc.: 85.00%] [G loss: 10.133579]\n",
      "212 [D loss: 0.033846, acc.: 100.00%] [G loss: 15.235046]\n",
      "213 [D loss: 0.827253, acc.: 90.00%] [G loss: 15.172264]\n",
      "214 [D loss: 2.248548, acc.: 80.00%] [G loss: 13.197614]\n",
      "215 [D loss: 2.145164, acc.: 80.00%] [G loss: 10.571199]\n",
      "216 [D loss: 0.166650, acc.: 95.00%] [G loss: 13.162814]\n",
      "217 [D loss: 1.123121, acc.: 80.00%] [G loss: 11.164810]\n",
      "218 [D loss: 0.273095, acc.: 95.00%] [G loss: 13.238527]\n",
      "219 [D loss: 0.891596, acc.: 80.00%] [G loss: 12.289732]\n",
      "220 [D loss: 0.055170, acc.: 95.00%] [G loss: 13.294241]\n",
      "221 [D loss: 1.151971, acc.: 85.00%] [G loss: 14.217636]\n",
      "222 [D loss: 1.675025, acc.: 75.00%] [G loss: 16.118095]\n",
      "223 [D loss: 2.943442, acc.: 60.00%] [G loss: 12.246164]\n",
      "224 [D loss: 0.748855, acc.: 85.00%] [G loss: 11.287021]\n",
      "225 [D loss: 1.146502, acc.: 85.00%] [G loss: 13.740300]\n",
      "226 [D loss: 0.223682, acc.: 95.00%] [G loss: 16.118095]\n",
      "227 [D loss: 0.960288, acc.: 85.00%] [G loss: 16.118095]\n",
      "228 [D loss: 1.887341, acc.: 80.00%] [G loss: 13.902086]\n",
      "229 [D loss: 0.206099, acc.: 95.00%] [G loss: 11.887952]\n",
      "230 [D loss: 0.736306, acc.: 90.00%] [G loss: 14.507129]\n",
      "231 [D loss: 1.166511, acc.: 85.00%] [G loss: 11.049322]\n",
      "232 [D loss: 1.272014, acc.: 85.00%] [G loss: 13.430364]\n",
      "233 [D loss: 0.444604, acc.: 95.00%] [G loss: 12.506162]\n",
      "234 [D loss: 2.181031, acc.: 70.00%] [G loss: 9.731209]\n",
      "235 [D loss: 1.433859, acc.: 70.00%] [G loss: 11.415490]\n",
      "236 [D loss: 0.000644, acc.: 100.00%] [G loss: 12.749156]\n",
      "237 [D loss: 0.256197, acc.: 95.00%] [G loss: 16.100523]\n",
      "238 [D loss: 0.023578, acc.: 100.00%] [G loss: 14.426511]\n",
      "239 [D loss: 1.509469, acc.: 75.00%] [G loss: 14.563406]\n",
      "240 [D loss: 1.341572, acc.: 75.00%] [G loss: 10.315641]\n",
      "241 [D loss: 1.050160, acc.: 90.00%] [G loss: 11.157765]\n",
      "242 [D loss: 0.577684, acc.: 90.00%] [G loss: 12.917529]\n",
      "243 [D loss: 0.644297, acc.: 80.00%] [G loss: 11.380341]\n",
      "244 [D loss: 0.360913, acc.: 95.00%] [G loss: 14.444972]\n",
      "245 [D loss: 0.470194, acc.: 95.00%] [G loss: 12.742861]\n",
      "246 [D loss: 0.692561, acc.: 80.00%] [G loss: 12.985971]\n",
      "247 [D loss: 0.362286, acc.: 95.00%] [G loss: 13.770517]\n",
      "248 [D loss: 0.108842, acc.: 95.00%] [G loss: 11.509766]\n",
      "249 [D loss: 0.126843, acc.: 90.00%] [G loss: 11.681788]\n",
      "250 [D loss: 1.625772, acc.: 90.00%] [G loss: 13.614641]\n",
      "251 [D loss: 1.574979, acc.: 80.00%] [G loss: 12.614726]\n",
      "252 [D loss: 0.978577, acc.: 90.00%] [G loss: 13.484964]\n",
      "253 [D loss: 2.700521, acc.: 55.00%] [G loss: 14.898836]\n",
      "254 [D loss: 1.479097, acc.: 85.00%] [G loss: 14.374103]\n",
      "255 [D loss: 0.239196, acc.: 90.00%] [G loss: 12.895541]\n",
      "256 [D loss: 0.475956, acc.: 95.00%] [G loss: 13.515208]\n",
      "257 [D loss: 1.171610, acc.: 85.00%] [G loss: 14.506534]\n",
      "258 [D loss: 1.680357, acc.: 85.00%] [G loss: 11.323082]\n",
      "259 [D loss: 0.762924, acc.: 80.00%] [G loss: 15.856290]\n",
      "260 [D loss: 0.803965, acc.: 90.00%] [G loss: 12.895834]\n",
      "261 [D loss: 0.567251, acc.: 90.00%] [G loss: 15.999365]\n",
      "262 [D loss: 0.222791, acc.: 90.00%] [G loss: 16.019762]\n",
      "263 [D loss: 2.253620, acc.: 75.00%] [G loss: 13.700200]\n",
      "264 [D loss: 0.718399, acc.: 95.00%] [G loss: 13.617167]\n",
      "265 [D loss: 0.119760, acc.: 95.00%] [G loss: 13.511762]\n",
      "266 [D loss: 0.941755, acc.: 90.00%] [G loss: 14.188870]\n",
      "267 [D loss: 0.000061, acc.: 100.00%] [G loss: 12.472834]\n",
      "268 [D loss: 0.118084, acc.: 95.00%] [G loss: 12.665991]\n",
      "269 [D loss: 0.809754, acc.: 95.00%] [G loss: 14.506686]\n",
      "270 [D loss: 1.218133, acc.: 90.00%] [G loss: 13.584116]\n",
      "271 [D loss: 1.131148, acc.: 85.00%] [G loss: 13.111540]\n",
      "272 [D loss: 0.032987, acc.: 100.00%] [G loss: 11.801843]\n",
      "273 [D loss: 1.072150, acc.: 80.00%] [G loss: 16.118095]\n",
      "274 [D loss: 0.589953, acc.: 90.00%] [G loss: 11.209941]\n",
      "275 [D loss: 0.000132, acc.: 100.00%] [G loss: 11.297431]\n",
      "276 [D loss: 0.707583, acc.: 95.00%] [G loss: 16.118095]\n",
      "277 [D loss: 0.066891, acc.: 95.00%] [G loss: 12.332554]\n",
      "278 [D loss: 1.220191, acc.: 75.00%] [G loss: 10.539957]\n",
      "279 [D loss: 1.145784, acc.: 90.00%] [G loss: 12.900650]\n",
      "280 [D loss: 0.797120, acc.: 95.00%] [G loss: 12.894786]\n",
      "281 [D loss: 1.749716, acc.: 75.00%] [G loss: 15.632074]\n",
      "282 [D loss: 1.629527, acc.: 75.00%] [G loss: 14.909277]\n",
      "283 [D loss: 0.163744, acc.: 90.00%] [G loss: 14.472392]\n",
      "284 [D loss: 1.319845, acc.: 80.00%] [G loss: 12.937113]\n",
      "285 [D loss: 0.063137, acc.: 95.00%] [G loss: 14.156453]\n",
      "286 [D loss: 0.073817, acc.: 95.00%] [G loss: 16.118095]\n",
      "287 [D loss: 0.599241, acc.: 95.00%] [G loss: 16.118095]\n",
      "288 [D loss: 1.191015, acc.: 85.00%] [G loss: 13.466036]\n",
      "289 [D loss: 0.829318, acc.: 85.00%] [G loss: 13.636332]\n",
      "290 [D loss: 0.526330, acc.: 85.00%] [G loss: 12.900836]\n",
      "291 [D loss: 0.044721, acc.: 100.00%] [G loss: 16.118095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292 [D loss: 0.021783, acc.: 100.00%] [G loss: 15.041229]\n",
      "293 [D loss: 0.349950, acc.: 95.00%] [G loss: 16.118095]\n",
      "294 [D loss: 0.574266, acc.: 95.00%] [G loss: 14.513570]\n",
      "295 [D loss: 0.558528, acc.: 90.00%] [G loss: 15.592539]\n",
      "296 [D loss: 0.000314, acc.: 100.00%] [G loss: 15.091593]\n",
      "297 [D loss: 0.245210, acc.: 90.00%] [G loss: 14.851176]\n",
      "298 [D loss: 0.522332, acc.: 85.00%] [G loss: 16.118095]\n",
      "299 [D loss: 0.450131, acc.: 85.00%] [G loss: 15.068715]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from numpy import random\n",
    "\n",
    "import scipy\n",
    "\n",
    "from scipy import misc\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.chdir(r'C:\\\\users\\\\sarva\\\\Projects\\\\GAN_face\\\\')\n",
    "\n",
    "\n",
    "\n",
    "class GAN():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.img_rows = 256\n",
    "\n",
    "        self.img_cols = 256\n",
    "\n",
    "        self.channels = 1\n",
    "\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        self.latent_dim = 100\n",
    "\n",
    "\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "\n",
    "            optimizer=optimizer,\n",
    "\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "        # Build the generator\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "\n",
    "        img = self.generator(z)\n",
    "\n",
    "\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "\n",
    "        # Trains the generator to fool the discriminator\n",
    "\n",
    "        self.combined = Model(z, validity)\n",
    "\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(512))\n",
    "\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(1024))\n",
    "\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "\n",
    "        img = model(noise)\n",
    "\n",
    "\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "\n",
    "        model.add(Dense(512))\n",
    "\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dense(256))\n",
    "\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        validity = model(img)\n",
    "\n",
    "\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "\n",
    "\n",
    "        # Load the dataset\n",
    "\n",
    "        #(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        directory = r\"C:\\\\users\\\\sarva\\\\Projects\\\\GAN_face\\\\person\\\\\"\n",
    "\n",
    "        training_data = np.empty([983,256,256])\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "        \n",
    "            training_data[i] = np.array(ndimage.imread(directory+file, flatten=False))\n",
    "            \n",
    "        \n",
    "            i = i + 1\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "\n",
    "        X_train = training_data / 127.5 - 1.\n",
    "\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "\n",
    "\n",
    "        # Adversarial ground truths\n",
    "\n",
    "        valid = np.ones((batch_size, 1))\n",
    "\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "\n",
    "            #  Train Discriminator\n",
    "\n",
    "            # ---------------------\n",
    "\n",
    "\n",
    "\n",
    "            # Select a random batch of images\n",
    "\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "\n",
    "\n",
    "            # Generate a batch of new images\n",
    "\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "\n",
    "\n",
    "            # Train the discriminator\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "\n",
    "            #  Train Generator\n",
    "\n",
    "            # ---------------------\n",
    "\n",
    "\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "\n",
    "\n",
    "            # Plot the progress\n",
    "\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "\n",
    "            if epoch % sample_interval == 0:\n",
    "\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "\n",
    "        r, c = 5, 5\n",
    "\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        for i in range(r):\n",
    "\n",
    "            for j in range(c):\n",
    "\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "\n",
    "                axs[i,j].axis('off')\n",
    "\n",
    "                cnt += 1\n",
    "\n",
    "        fig.savefig(\"GAN_images/%d.png\" % epoch)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    gan = GAN()\n",
    "\n",
    "    gan.train(epochs=300, batch_size=10, sample_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarva\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:214: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "C:\\Users\\sarva\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.270708, acc.: 95.00%] [G loss: 14.506289]\n"
     ]
    }
   ],
   "source": [
    "gan.train(epochs=1, batch_size=10, sample_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sample_images(self, epoch):\n",
    "\n",
    "        noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
    "\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        fig.savefig(\"GAN_images/%d.png\" % epoch)\n",
    "\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_validate_lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-953348653463>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\skimage\\transform\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from .hough_transform import (hough_line, hough_line_peaks,\n\u001b[0m\u001b[0;32m      2\u001b[0m                               \u001b[0mprobabilistic_hough_line\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhough_circle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                               hough_circle_peaks, hough_ellipse)\n\u001b[0;32m      4\u001b[0m from .radon_transform import (radon, iradon, iradon_sart,\n\u001b[0;32m      5\u001b[0m                               order_angles_golden_ratio)\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\skimage\\transform\\hough_transform.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m from ._hough_transform import (_hough_circle,\n\u001b[0;32m      5\u001b[0m                                \u001b[0m_hough_ellipse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\skimage\\measure\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_find_contours\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfind_contours\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_marching_cubes_lewiner\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmarching_cubes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarching_cubes_lewiner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m from ._marching_cubes_classic import (marching_cubes_classic,\n\u001b[0;32m      4\u001b[0m                                       \u001b[0mmesh_surface_area\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                       correct_mesh_orientation)\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\skimage\\measure\\_marching_cubes_lewiner.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_marching_cubes_lewiner_luts\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcluts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_marching_cubes_lewiner_cy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskimage_deprecation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\skimage\\_shared\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_as_float\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_warnings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mall_warnings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\skimage\\util\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mapply_parallel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapply_parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0marraycrop\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_regular_grid\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregular_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregular_seeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munique_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\skimage\\util\\arraycrop.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marraypad\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_validate_lengths\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_validate_lengths'"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "from scipy import ndarray\n",
    "\n",
    "import skimage as sk\n",
    "\n",
    "from skimage import transform\n",
    "\n",
    "from skimage import util\n",
    "\n",
    "\n",
    "\n",
    "def random_rotation(image_array: ndarray):\n",
    "\n",
    "    # pick a random degree of rotation between 25% on the left and 25% on the right\n",
    "\n",
    "    random_degree = random.uniform(-25, 25)\n",
    "\n",
    "    return sk.transform.rotate(image_array, random_degree)\n",
    "\n",
    "\n",
    "\n",
    "def random_noise(image_array: ndarray):\n",
    "\n",
    "    # add random noise to the image\n",
    "\n",
    "    return sk.util.random_noise(image_array)\n",
    "\n",
    "\n",
    "\n",
    "def horizontal_flip(image_array: ndarray):\n",
    "\n",
    "    # horizontal flip doesn't need skimage, it's easy as flipping the image array of pixels !\n",
    "\n",
    "    return image_array[:, ::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_pixels = []\n",
    "\n",
    "for Pic in os.listdir('person'):\n",
    "    fname = 'person/' + Pic\n",
    "    image = np.array(ndimage.imread(fname, flatten=False))\n",
    "    my_image = scipy.misc.imresize(image, size=(square_size, square_size)).reshape(square_size*square_size,1)\n",
    "    person_pixels.append(my_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
